// TESI: Integrazione FPGA in FastFlow

Si propone di realizzare un semplice prototipo per l'integrazione di kernel programmati su FPGA Alveo mediante Vitis (sostanzialmente kernel scritti in C/C++) nell'ambiente di programmazione FastFlow. Verranno utilizzati kernel esistenti.
L'obiettivo della tesi sarà quello di interfacciare i kernel mediante codice OpenCL sfruttando tutte le possibilità offerte da tale ambiente. L'integrazione in FastFlow dovrà permettere di utilizzare sia il classico parallelismo shared memory/thread sulla CPU multicore che l'offloading di computazioni pesanti sull'acceleratore FPGA.


FastFlow:
   https://github.com/fastflow/fastflow

Parte di colloquio con l'acceleratore vedi VITIS XILINX:
	https://docs.amd.com/r/en-US/Vitis-Tutorials-Getting-Started/Vitis-Introduction-and-Getting-Started

Per usarla con GPU
    https://eunomia.dev/en/others/cuda-tutorial/15-opencl-vector-addition/

L'account è deleonardis, la macchina è
	pianosa.di.unipi.it has address 131.114.3.250




9/09 -> CALCOLO SEQUENZIALE




DONE!!!!!! ----- DEVO ANCORA RIUTILIZZARE I BUFFER di memoria sulla GPU/FPGA (su CPU sono riutilizzati):
   Spostare l'Allocazione: Spostare le chiamate clCreateBuffer dal metodo execute() al metodo initialize() degli adapter. I cl_mem (bufferA, bufferB, bufferC) diventerebbero membri privati delle classi GpuAccelerator e FpgaAccelerator.
   Semplificare execute: Il metodo execute() non dovrebbe più allocare e deallocare, ma solo orchestrare i trasferimenti di dati (Write, Read) e l'esecuzione del kernel, usando i buffer pre-allocati.
   Spostare la Deallocazione: Le chiamate clReleaseMemObject andrebbero spostate nel distruttore degli adapter.

   L'obiettivo è eliminare le costose operazioni di allocazione (clCreateBuffer) e deallocazione (clReleaseMemObject) della memoria per ogni singolo task, riutilizzando invece gli stessi buffer pre-allocati.




Punti chiave da ricordare:
   Asincronia: la computazione sull’acceleratore non blocca i thread FastFlow grazie alla coppia Producer/Consumer.
   Overlapping: mentre il kernel n è in esecuzione, il producer può preparare i dati per n + 1 e il consumer può leggere i risultati di n – 1.
   Portabilità: il flusso è identico per GPU (Apple) e FPGA (Xilinx); cambiano solo le funzioni specifiche usate dentro DeviceAdapter.



// BUILD AND EXEC:
   rm -rf build; cmake -B build && cmake --build build && ./build/tesi-exec 1000000 2 gpu

// Install correct FF Version on VM:
   rm -rf external/fastflow; git clone https://github.com/fastflow/fastflow.git external/fastflow

// SYNC Mac with VM
   rsync -av --exclude 'build/' --exclude '.DS_Store'  /Users/davidedeleonardis/Desktop/Tesi/ deleonardis@131.114.3.250:~/Tesi/

// COMPILAZIONE E LINKING SU FPGA per ottenere xclbin (in Tesi)
v++ -c -k krnl_vadd --platform xilinx_u50_gen3x16_xdma_5_202210_1 --target hw -o kernel/krnl_vadd.xo kernel/krnl_vadd.cpp
v++ -l --platform xilinx_u50_gen3x16_xdma_5_202210_1 --target hw -o krnl_vadd.xclbin kernel/krnl_vadd.xo





CON 100 TASK: il costo di setup dell'acceleratore viene ammortizzato, rendendo la soluzione GPU vincente in uno scenario a throughput elevato => OK!!!!
   L'overhead medio per task nella versione a 100 task (919 µs) è crollato rispetto a quello del singolo task (73,827 µs). 
   Questo dimostra numericamente che l'enorme costo di setup iniziale (la compilazione del kernel, etc.) è stato "spalmato" con successo sui 100 task. 
   L'overhead rimanente è ora praticamente identico a quello della versione CPU, rappresentando il costo "puro" del framework FastFlow.




1. Efficienza della Baseline CPU
I dati della CPU mostrano che l'overhead di FastFlow è minimo e costante (circa 900-1000 µs). 
Per problemi grandi, il tempo elapsed è quasi interamente tempo computed. 
Questo significa che la nostra baseline è solida e il sistema è limitato solo dalla velocità del calcolo (è compute-bound).


2. Il Calcolo su GPU non è Sempre Vantaggioso (computed_time)
Break-Even Point: Per problemi piccoli (N=1M), il computed_time della GPU è peggiore di quello della CPU. 
Questo è un risultato classico: il tempo risparmiato grazie al parallelismo della GPU è inferiore al tempo speso per trasferire i dati avanti e indietro sulla memoria del dispositivo. 
Il sistema raggiunge il "break-even" e diventa vantaggioso solo per N sufficientemente grandi.

Scalabilità non Lineare: Lo speedup del calcolo non aumenta linearmente con N. Passando da 16M a 67M elementi, lo speedup diminuisce da 1.89x a 1.20x. Questo è un punto di analisi molto interessante per la tesi. Possibili cause:
   - Il task (somma vettoriale) è molto semplice e limita la banda di memoria (memory-bound). 
   Oltre una certa dimensione, sia la CPU che la GPU sono limitate non dalla loro potenza di calcolo, ma dalla velocità con cui possono leggere e scrivere dalla memoria.
   - La cache della CPU sull'M2 Pro è molto performante e potrebbe mitigare il vantaggio della GPU su problemi così lineari.


3. L'Overhead di Inizializzazione Domina il Tempo Totale (elapsed_time)
Questa è la conclusione più importante di questo esperimento. In tutti i casi, il tempo totale della versione GPU è peggiore di quello della CPU.
La colonna "Overhead" per la GPU è enorme e relativamente costante (70-95 millisecondi).
Come avevamo ipotizzato, questo costo è quasi interamente dovuto all'inizializzazione una tantum di OpenCL in svc_init, in particolare alla compilazione JIT (Just-In-Time) del kernel (clBuildProgram).
Poiché stiamo eseguendo un singolo task, questo costo fisso iniziale non viene mai ammortizzato e domina completamente il tempo di esecuzione totale. 


4. Lezione sulla Legge di Amdahl e l'Ammortamento
Questi risultati sono una perfetta dimostrazione pratica della Legge di Amdahl. La parte "seriale" del nostro programma (l'inizializzazione di OpenCL) è così lenta che, anche se acceleriamo la parte "parallela" (il calcolo), il guadagno totale è negativo per un singolo task.
Questo dimostra in modo conclusivo che l'offloading su acceleratori ha senso solo in scenari ad alto throughput, dove il costo di setup viene "spalmato" su un gran numero di operazioni.







Avvia sessione:
   screen -S vitis

Resume sessione:
   screen -r vitis   

Per ritornare in terminale da sessione:
   ctrl-a
   d



// L'Analisi del Problema - N troppo grande (PROBLEMA ANCHE IN FPGA)
The problem is the massive size of N you're trying to process in a single go. Let's do the math:
N (Dimensione del Vettore): 1,000,000,000 (1 miliardo di elementi)
Tipo di Dato: int (4 byte per elemento)
Numero di Vettori: 3 (input a, input b, output c)
The total memory you're asking the GPU to allocate is:
3 * 1,000,000,000 * 4 byte = 12,000,000,000 byte = 12 Gigabyte (GB).
Your Mac M2 Pro has a unified memory architecture (e.g., 16 GB or 32 GB total), but that memory is shared between the CPU, the GPU, the operating system, and all other running applications. 
Attempting to allocate a single chunk of 12 GB for your application is exceeding the amount of memory available to the GPU at that moment, causing the allocation to fail.

=> con n troppo grande la cpu dell'fpga ha spazio a disposizione per calcolare, MA fpga NO
   => FPGA ha 8GB di memoria a disposizione quindi dovrebbe essere in grado di calcolare con n=67108864 (805MB da utilizzare) ma l'OS non me lo permette
         xbutil examine --device 0000:81:00.1 --report memory







L'architettura a 3 stadi con questo nodo asincrono è instabile in FastFlow => uso 2 stadi (Emitter, accNode)




2. La Gerarchia delle Performance per la Somma Vettoriale
Per questo specifico task (somma vettoriale), i tuoi dati mostrano una chiara gerarchia di performance in termini di throughput:
   GPU > CPU > FPGA
Questo è un risultato controintuitivo e molto valido. Dimostra che non sempre l'hardware più specializzato è il più veloce e che la natura del problema è fondamentale.

3. Analisi: "Il Martello Giusto per il Chiodo Giusto"
Questo è il cuore della tua analisi. Perché l'FPGA, un acceleratore hardware, è risultata più lenta della CPU?
Task Memory-Bound: La somma vettoriale è un'operazione estremamente semplice, limitata non dalla potenza di calcolo, ma dalla velocità con cui si possono leggere e scrivere dati dalla memoria (memory-bound).
Ottimizzazione di CPU e GPU: Le CPU e le GPU moderne sono macchine potentissime e altamente ottimizzate per questo tipo di accesso lineare alla memoria, con cache sofisticate e bus di memoria molto ampi.
Forza dell'FPGA: La vera forza di un'FPGA non è eseguire task semplici, ma implementare pipeline di calcolo complesse e customizzate che non esistono su una CPU/GPU. Ad esempio, algoritmi di compressione video, elaborazione di segnali radio, crittografia, dove i dati fluiscono attraverso una catena di montaggio hardware su misura. Per un task così semplice, l'overhead di comunicazione con la scheda e la sua architettura di memoria generica non riescono a competere.
Usare un'FPGA per una somma vettoriale è come usare un bisturi chirurgico specializzato per aprire una scatola di cartone: funziona, ma un semplice taglierino (la CPU/GPU) è più efficiente per quel compito specifico.

4. Overhead di Inizializzazione
I tuoi log mostrano che anche l'FPGA ha un costo di inizializzazione significativo (elapsed - computed nel primo task), dovuto al tempo necessario per caricare il file .xclbin sulla scheda e configurare il circuito. Hai dimostrato che, come per la GPU, questo costo viene ammortizzato solo in scenari a throughput elevato.











Se volessi sostituire la somma vettoriale con un altro kernel, per esempio una moltiplicazione tra matrici, non dovresti toccare l'infrastruttura principale del progetto (ff_node_acc_t, la logica della pipeline, etc.). Le modifiche sarebbero mirate e contenute solo nelle parti specifiche del task.

File da Modificare
   1. I Nuovi File dei Kernel (L'Algoritmo)
   Prima di tutto, dovresti scrivere il nuovo algoritmo. Non modificheresti i file esistenti, ma ne creeresti di nuovi.
   Per la GPU: Creeresti un nuovo file, ad esempio matrix_mul.cl, contenente il kernel OpenCL C per la moltiplicazione di matrici.
   Per l'FPGA: Creeresti un nuovo file C++, ad esempio kernel/krnl_matmul.cpp, con la logica per la sintesi HLS della moltiplicazione di matrici.

   2. Il File dei Tipi Dati (include/types.hpp)
   La struttura Task attuale è specifica per la somma vettoriale (tre vettori, una dimensione). Una moltiplicazione di matrici ha bisogno di dati diversi.
   Azione: Aggiungeresti una nuova struct per descrivere il nuovo lavoro, per esempio:
   // Esempio per una moltiplicazione di matrici C = A x B
   struct MatrixMulTask {
      float *matrix_a, *matrix_b, *matrix_c;
      int width_a, height_a, width_b;
   };
   Anche la struct Result potrebbe dover cambiare se il risultato fosse più complesso.

   3. Le Classi "Adapter" (Cpu/Gpu/FpgaAccelerator.cpp)
   Qui è dove "collegheresti" il nuovo task e il nuovo kernel. Ogni adapter andrebbe modificato.
   CpuAccelerator.cpp:
   Nel metodo execute, la chiamata a std::transform verrebbe sostituita con due cicli for annidati per eseguire la moltiplicazione di matrici sulla CPU.
   GpuAccelerator.cpp e FpgaAccelerator.cpp:
   Nel metodo initialize, cambieresti il nome del file del kernel da caricare (es. matrix_mul.cl o il nuovo .xclbin) e il nome del kernel da estrarre (es. "matrix_mul").
   Nel metodo execute, faresti le modifiche più sostanziose:
   Faresti il cast del puntatore generic_task al nuovo tipo MatrixMulTask*.
   Le chiamate clCreateBuffer creerebbero buffer con le dimensioni corrette per le matrici.
   Le chiamate clSetKernelArg passerebbero i puntatori alle matrici e le loro dimensioni, in accordo con la firma del nuovo kernel.

   4. La Classe Emitter (in main.cpp)
   L'Emitter è responsabile di creare i dati di test.
   Azione: Modificheresti il costruttore dell'Emitter per inizializzare due matrici di input (a e b) con valori sensati. Anche la chiamata per creare il task diventerebbe return new MatrixMulTask{...};.


File che NON Andrebbero Modificati (La Prova della Buona Architettura)
   IAccelerator.hpp: L'interfaccia, il "contratto", non cambia. Definisce ancora le azioni generiche initialize ed execute.
   ff_node_acc_t.hpp e ff_node_acc_t.cpp: Il tuo "motore" asincrono a thread e code rimarrebbe identico. Non gli interessa quale calcolo venga eseguito; il suo compito è solo orchestrare il flusso di void* tra le code e delegare il lavoro all'acceleratore.
   La funzione main() in main.cpp: La logica di creazione della pipeline, la misurazione dei tempi e il meccanismo di verifica con promise/future rimarrebbero esattamente gli stessi. L'unica riga da cambiare sarebbe quella che crea l'adapter specifico (es. accelerator = std::make_unique<MatrixMulGpuAccelerator>();).




1. Tempo Elapsed (elapsed / NUM_TASKS = throughput -> costo medio per completare un singolo task all'interno di un'esecuzione continua)
Questo è il tempo completo che la tua pipeline impiega per processare tutti i task, dall'inizio alla fine. È la metrica che misura le performance dell'intero sistema software.
Cosa include:
   Il tempo di gestione della pipeline FastFlow.
   Il tempo che i Task e i Result passano nelle code (inQ_, outQ_).
   Il tempo di creazione e distruzione degli oggetti Task e Result.
   Il tempo di trasferimento dei dati dalla CPU all'acceleratore (per GPU e FPGA).
   Il tempo di calcolo vero e proprio (computed_time).
   Il tempo di trasferimento dei risultati dall'acceleratore alla CPU.

2. Tempo Computed (Tempo di Calcolo Effettivo)
Questo è il tempo speso esclusivamente dall'acceleratore per eseguire il calcolo. Misura la performance pura del "motore" di calcolo, isolandolo da tutto il resto dell'infrastruttura software.
Cosa include (per GPU e FPGA):
   Il tempo per copiare i dati dalla RAM del computer alla memoria della scheda (clEnqueueWriteBuffer).
   Il tempo per eseguire il kernel sulla scheda (clEnqueueNDRangeKernel).
   Il tempo per ricopiare i risultati dalla scheda alla RAM (clEnqueueReadBuffer).
   Il tempo per sincronizzare e assicurarsi che tutte le operazioni siano finite (clFinish).

