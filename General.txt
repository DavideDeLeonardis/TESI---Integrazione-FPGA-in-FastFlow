// TESI: Integrazione FPGA in FastFlow

Si propone di realizzare un semplice prototipo per l'integrazione di kernel programmati su FPGA Alveo mediante Vitis (sostanzialmente kernel scritti in C/C++) nell'ambiente di programmazione FastFlow. Verranno utilizzati kernel esistenti.
L'obiettivo della tesi sarà quello di interfacciare i kernel mediante codice OpenCL sfruttando tutte le possibilità offerte da tale ambiente. L'integrazione in FastFlow dovrà permettere di utilizzare sia il classico parallelismo shared memory/thread sulla CPU multicore che l'offloading di computazioni pesanti sull'acceleratore FPGA.


FastFlow:
   https://github.com/fastflow/fastflow

Parte di colloquio con l'acceleratore vedi VITIS XILINX:
	https://docs.amd.com/r/en-US/Vitis-Tutorials-Getting-Started/Vitis-Introduction-and-Getting-Started

Per usarla con GPU
    https://eunomia.dev/en/others/cuda-tutorial/15-opencl-vector-addition/

L'account è deleonardis, la macchina è
	pianosa.di.unipi.it has address 131.114.3.250




DEVO ANCORA RIUTILIZZARE I BUFFER di memoria sulla GPU/FPGA (su CPU sono riutilizzati):
   Spostare l'Allocazione: Spostare le chiamate clCreateBuffer dal metodo execute() al metodo initialize() degli adapter. I cl_mem (bufferA, bufferB, bufferC) diventerebbero membri privati delle classi GpuAccelerator e FpgaAccelerator.
   Semplificare execute: Il metodo execute() non dovrebbe più allocare e deallocare, ma solo orchestrare i trasferimenti di dati (Write, Read) e l'esecuzione del kernel, usando i buffer pre-allocati.
   Spostare la Deallocazione: Le chiamate clReleaseMemObject andrebbero spostate nel distruttore degli adapter.

   L'obiettivo è eliminare le costose operazioni di allocazione (clCreateBuffer) e deallocazione (clReleaseMemObject) della memoria per ogni singolo task, riutilizzando invece gli stessi buffer pre-allocati.




Punti chiave da ricordare:
   Asincronia: la computazione sull’acceleratore non blocca i thread FastFlow grazie alla coppia Producer/Consumer.
   Overlapping: mentre il kernel n è in esecuzione, il producer può preparare i dati per n + 1 e il consumer può leggere i risultati di n – 1.
   Portabilità: il flusso è identico per GPU (Apple) e FPGA (Xilinx); cambiano solo le funzioni specifiche usate dentro DeviceAdapter.



// BUILD AND EXEC:
rm -rf build; cmake -B build && cmake --build build && ./build/tesi-exec 1000000 2 gpu


Per VM - FF version:
   rm -rf external/fastflow; git clone https://github.com/fastflow/fastflow.git external/fastflow




CON 100 TASK: il costo di setup dell'acceleratore viene ammortizzato, rendendo la soluzione GPU vincente in uno scenario a throughput elevato => OK!!!!
   L'overhead medio per task nella versione a 100 task (919 µs) è crollato rispetto a quello del singolo task (73,827 µs). 
   Questo dimostra numericamente che l'enorme costo di setup iniziale (la compilazione del kernel, etc.) è stato "spalmato" con successo sui 100 task. 
   L'overhead rimanente è ora praticamente identico a quello della versione CPU, rappresentando il costo "puro" del framework FastFlow.




1. Efficienza della Baseline CPU
I dati della CPU mostrano che l'overhead di FastFlow è minimo e costante (circa 900-1000 µs). 
Per problemi grandi, il tempo elapsed è quasi interamente tempo computed. 
Questo significa che la nostra baseline è solida e il sistema è limitato solo dalla velocità del calcolo (è compute-bound).


2. Il Calcolo su GPU non è Sempre Vantaggioso (computed_time)
Break-Even Point: Per problemi piccoli (N=1M), il computed_time della GPU è peggiore di quello della CPU. 
Questo è un risultato classico: il tempo risparmiato grazie al parallelismo della GPU è inferiore al tempo speso per trasferire i dati avanti e indietro sulla memoria del dispositivo. 
Il sistema raggiunge il "break-even" e diventa vantaggioso solo per N sufficientemente grandi.

Scalabilità non Lineare: Lo speedup del calcolo non aumenta linearmente con N. Passando da 16M a 67M elementi, lo speedup diminuisce da 1.89x a 1.20x. Questo è un punto di analisi molto interessante per la tesi. Possibili cause:
   - Il task (somma vettoriale) è molto semplice e limita la banda di memoria (memory-bound). 
   Oltre una certa dimensione, sia la CPU che la GPU sono limitate non dalla loro potenza di calcolo, ma dalla velocità con cui possono leggere e scrivere dalla memoria.
   - La cache della CPU sull'M2 Pro è molto performante e potrebbe mitigare il vantaggio della GPU su problemi così lineari.


3. L'Overhead di Inizializzazione Domina il Tempo Totale (elapsed_time)
Questa è la conclusione più importante di questo esperimento. In tutti i casi, il tempo totale della versione GPU è peggiore di quello della CPU.
La colonna "Overhead" per la GPU è enorme e relativamente costante (70-95 millisecondi).
Come avevamo ipotizzato, questo costo è quasi interamente dovuto all'inizializzazione una tantum di OpenCL in svc_init, in particolare alla compilazione JIT (Just-In-Time) del kernel (clBuildProgram).
Poiché stiamo eseguendo un singolo task, questo costo fisso iniziale non viene mai ammortizzato e domina completamente il tempo di esecuzione totale. 


4. Lezione sulla Legge di Amdahl e l'Ammortamento
Questi risultati sono una perfetta dimostrazione pratica della Legge di Amdahl. La parte "seriale" del nostro programma (l'inizializzazione di OpenCL) è così lenta che, anche se acceleriamo la parte "parallela" (il calcolo), il guadagno totale è negativo per un singolo task.
Questo dimostra in modo conclusivo che l'offloading su acceleratori ha senso solo in scenari ad alto throughput, dove il costo di setup viene "spalmato" su un gran numero di operazioni.







// COMPILAZIONE E LINKING SU FPGA per ottenere xclbin (in Tesi)
v++ -c -k krnl_vadd --platform xilinx_u50_gen3x16_xdma_5_202210_1 --target hw -o kernel/krnl_vadd.xo kernel/krnl_vadd.cpp
v++ -l --platform xilinx_u50_gen3x16_xdma_5_202210_1 --target hw -o krnl_vadd.xclbin kernel/krnl_vadd.xo




Avvia sessione:
   screen -S vitis

Resume sessione:
   screen -r vitis   

Per ritornare in terminale da sessione:
   ctrl-a
   d



// L'Analisi del Problema - N troppo grande (PROBLEMA ANCHE IN FPGA)
The problem is the massive size of N you're trying to process in a single go. Let's do the math:
N (Dimensione del Vettore): 1,000,000,000 (1 miliardo di elementi)
Tipo di Dato: int (4 byte per elemento)
Numero di Vettori: 3 (input a, input b, output c)
The total memory you're asking the GPU to allocate is:
3 * 1,000,000,000 * 4 byte = 12,000,000,000 byte = 12 Gigabyte (GB).
Your Mac M2 Pro has a unified memory architecture (e.g., 16 GB or 32 GB total), but that memory is shared between the CPU, the GPU, the operating system, and all other running applications. 
Attempting to allocate a single chunk of 12 GB for your application is exceeding the amount of memory available to the GPU at that moment, causing the allocation to fail.

=> con n troppo grande la cpu dell'fpga ha spazio a disposizione per calcolare, MA fpga NO
   => FPGA ha 8GB di memoria a disposizione quindi dovrebbe essere in grado di calcolare con n=67108864 (805MB da utilizzare) ma l'OS non me lo permette
         xbutil examine --device 0000:81:00.1 --report memory







L'architettura a 3 stadi con questo nodo asincrono è instabile in FastFlow => uso 2 stadi (Emitter, accNode)




2. La Gerarchia delle Performance per la Somma Vettoriale
Per questo specifico task (somma vettoriale), i tuoi dati mostrano una chiara gerarchia di performance in termini di throughput:
GPU > CPU > FPGA

Questo è un risultato controintuitivo e molto valido. Dimostra che non sempre l'hardware più specializzato è il più veloce e che la natura del problema è fondamentale.

3. Analisi: "Il Martello Giusto per il Chiodo Giusto"
Questo è il cuore della tua analisi. Perché l'FPGA, un acceleratore hardware, è risultata più lenta della CPU?

Task Memory-Bound: La somma vettoriale è un'operazione estremamente semplice, limitata non dalla potenza di calcolo, ma dalla velocità con cui si possono leggere e scrivere dati dalla memoria (memory-bound).

Ottimizzazione di CPU e GPU: Le CPU e le GPU moderne sono macchine potentissime e altamente ottimizzate per questo tipo di accesso lineare alla memoria, con cache sofisticate e bus di memoria molto ampi.

Forza dell'FPGA: La vera forza di un'FPGA non è eseguire task semplici, ma implementare pipeline di calcolo complesse e customizzate che non esistono su una CPU/GPU. Ad esempio, algoritmi di compressione video, elaborazione di segnali radio, crittografia, dove i dati fluiscono attraverso una catena di montaggio hardware su misura. Per un task così semplice, l'overhead di comunicazione con la scheda e la sua architettura di memoria generica non riescono a competere.

Usare un'FPGA per una somma vettoriale è come usare un bisturi chirurgico specializzato per aprire una scatola di cartone: funziona, ma un semplice taglierino (la CPU/GPU) è più efficiente per quel compito specifico.

4. Overhead di Inizializzazione
I tuoi log mostrano che anche l'FPGA ha un costo di inizializzazione significativo (elapsed - computed nel primo task), dovuto al tempo necessario per caricare il file .xclbin sulla scheda e configurare il circuito. Hai dimostrato che, come per la GPU, questo costo viene ammortizzato solo in scenari a throughput elevato.